{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens - Freebase Processing\n",
    "#### Create Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "\n",
    "kg_path = 'datasets/www_data/www_data/Movielens/kg/train.dat'\n",
    "rec_path = 'datasets/www_data/www_data/Movielens/rs/ratings.txt'\n",
    "kg = np.genfromtxt(kg_path, delimiter='\\t', dtype=np.int32)\n",
    "rec = np.genfromtxt(rec_path, delimiter='\\t', dtype=np.int32)\n",
    "\n",
    "rec = rec[:,:3] # remove time col.\n",
    "\n",
    "rec[:,2] = rec[:,2] >= 4 # binary ratings, 0 if [0, 4), 1 if [4, 5] \n",
    "rec = rec[rec[:,2] == 1] # select only positive ratings\n",
    "rec[:,2] = 0 # set redundant col to relationship 0\n",
    "kg[:,1] += 1 # offset\n",
    "rec = rec[:, [0,2,1]] # <user, likes, item> format\n",
    "print(np.unique(rec[:, 1]))\n",
    "\n",
    "# checkpoint: user and item format are still in ml id's\n",
    "# step 1: convert item id's first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_FB_IDS = np.max(kg) # total number of default kg pairs (# rel << # entities)\n",
    "\n",
    "# paths for converting data\n",
    "item2kg_path = 'datasets/www_data/www_data/Movielens/rs/i2kg_map.tsv'\n",
    "emap_path = 'datasets/www_data/www_data/Movielens/kg/e_map.dat'\n",
    "\n",
    "# maps movie lense id's to free base html links\n",
    "ml2fb_map = {}\n",
    "with open(item2kg_path) as f:\n",
    "    for line in f:\n",
    "        ml_id = re.search('(.+?)\\t', line)\n",
    "        fb_http = re.search('\\t(.+?)\\n', line)\n",
    "        \n",
    "        ml2fb_map.update({int(ml_id.group(1)) : fb_http.group(1)})\n",
    "\n",
    "# maps free base html links to free base id's (final format)\n",
    "id2html_map = {}\n",
    "fb2id_map = {}\n",
    "with open(emap_path) as f:\n",
    "    for kg_id, line in enumerate(f):\n",
    "        fb_http = re.search('\\t(.+?)\\n', line)\n",
    "        \n",
    "        fb2id_map.update({fb_http.group(1) : kg_id})\n",
    "        id2html_map.update({kg_id : fb_http.group(1)})\n",
    "\n",
    "# convert movielens id's to freebase id's\n",
    "i = 0\n",
    "while True:\n",
    "    if i == rec.shape[0]:\n",
    "        break\n",
    "\n",
    "    if rec[i,2] in ml2fb_map: \n",
    "        # get correct freebase id from data\n",
    "        fb_http = ml2fb_map[rec[i,2]]\n",
    "        fb_id = fb2id_map[fb_http]\n",
    "        # TODO: is this right\n",
    "        rec[i,2] = fb_id\n",
    "        i += 1\n",
    "    # remove from rec (only use movies that are in kg)\n",
    "    else:\n",
    "        rec = np.delete(rec, i, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_path = 'datasets/www_data/www_data/Movielens/rs/u_map.dat'\n",
    "\n",
    "# maps movielens user id's to freebase id's\n",
    "userid2fbid_map = {}\n",
    "new_ids = 0\n",
    "with open(umap_path) as f:\n",
    "    for line in f:\n",
    "\n",
    "        ml_id = re.search('\\t(.+?)\\n', line)\n",
    "        if int(ml_id.group(1)) in rec[:,0]:\n",
    "            new_ids += 1\n",
    "            userid2fbid_map.update({int(ml_id.group(1)) : TOTAL_FB_IDS + new_ids})\n",
    "\n",
    "# convert movielens user id's into freebase id's\n",
    "for i in range(rec.shape[0]):\n",
    "    rec[i,0] = userid2fbid_map[rec[i,0]]\n",
    "\n",
    "NEW_USER_IDS = new_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Dictionary for Triplet Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed 299 items\n"
     ]
    }
   ],
   "source": [
    "# create dictionary which maps id's to readable text\n",
    "fbid2word_map = {}\n",
    "item_path = 'datasets/www_data/www_data/Movielens/rs/i_map.dat'\n",
    "movie_path = 'datasets/www_data/www_data/Movielens/rs/movies.csv'\n",
    "\n",
    "# converts short movielens ids to movielens id\n",
    "movie_count = 0\n",
    "shortml2ml_map = {}\n",
    "with open(item_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        ml_id = re.search('\\t(.+?)\\n', line)\n",
    "        shortml2ml_map.update({i : int(ml_id.group(1))})\n",
    "        movie_count += 1\n",
    "\n",
    "shortml2movie_map = {}\n",
    "with open(movie_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0: continue # skip first line\n",
    "        movie = re.search(',(.+?),', line)\n",
    "        shortml2movie_map.update({i-1 : movie.group(1)})\n",
    "\n",
    "# add movies to dict\n",
    "miss_list = []\n",
    "for i in range(movie_count):\n",
    "    ml_id = shortml2ml_map[i]\n",
    "    if ml_id in ml2fb_map:\n",
    "        fb_http = ml2fb_map[ml_id]\n",
    "    else:\n",
    "        # items not in dict because no ratings at 4 or above\n",
    "        miss_list.append(ml_id)\n",
    "    fb_id = fb2id_map[fb_http]\n",
    "    movie = shortml2movie_map[i]\n",
    "\n",
    "    fbid2word_map.update({fb_id : movie})\n",
    "print('missed {} items'.format(len(miss_list)))\n",
    "\n",
    "# add freebase links that aren't movies\n",
    "# maps free base html links to free base id's (final format)\n",
    "emap_path = 'datasets/www_data/www_data/Movielens/kg/e_map.dat'\n",
    "id2http_map = {}\n",
    "with open(emap_path) as f:\n",
    "    for kg_id, line in enumerate(f):\n",
    "        fb_http = re.search('\\t(.+?)\\n', line).group(1)\n",
    "        \n",
    "        id2http_map.update({kg_id : fb_http})\n",
    "\n",
    "for i in range(TOTAL_FB_IDS):\n",
    "    # if not a movie\n",
    "    if i not in fbid2word_map:\n",
    "        fbid2word_map.update({i : id2http_map[i]})\n",
    "\n",
    "# add users to dict\n",
    "for i in range(NEW_USER_IDS):\n",
    "    fbid2word_map.update({TOTAL_FB_IDS + i + 1 : 'User {}'.format(i)})\n",
    "\n",
    "# relationship map\n",
    "link_map = {}\n",
    "link_path = 'datasets/www_data/www_data/Movielens/kg/r_map.dat'\n",
    "\n",
    "# make dict of relationship vals\n",
    "with open(link_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        link = re.search('/film.film.(.+?)>', line)\n",
    "        link_map.update({i : link.group(1).replace('_', ' ').capitalize()})\n",
    "link_map.update({47 : 'Likes'}) # add likes relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dictionary of User Likes Items and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# split rec and kg\n",
    "#np.random.shuffle(rec)\n",
    "#split = int(0.7*rec.shape[0])\n",
    "#rec_train = rec[:split]\n",
    "#rec_test = rec[split:]\n",
    "\n",
    "#np.random.shuffle(kg)\n",
    "#kg_test = kg[split:]\n",
    "path = 'datasets/ML_FB/'\n",
    "rec_test = np.load(os.path.join(path, 'rec_test.npy'), allow_pickle=True)\n",
    "rec_train = np.load(os.path.join(path, 'rec_train.npy'), allow_pickle=True)\n",
    "kg_test = np.load(os.path.join(path, 'kg_test.npy'), allow_pickle=True)\n",
    "kg_train = np.load(os.path.join(path, 'kg_train.npy'), allow_pickle=True)\n",
    "\n",
    "# user like maps\n",
    "user_likes_test = {}\n",
    "for i in range(rec_test.shape[0]):\n",
    "    if rec_test[i,0] not in user_likes_test:\n",
    "        user_likes_test.update({rec_test[i,0]: [rec_test[i,2]]})\n",
    "    else:\n",
    "        if rec_test[i,2] not in user_likes_test[rec_test[i,0]]:\n",
    "            user_likes_test[rec_test[i,0]].append(rec_test[i,2])\n",
    "\n",
    "user_likes_train = {}\n",
    "for i in range(rec_train.shape[0]):\n",
    "    if rec_train[i,0] not in user_likes_train:\n",
    "        user_likes_train.update({rec_train[i,0]: [rec_train[i,2]]})\n",
    "    else:\n",
    "        if rec_train[i,2] not in user_likes_train[rec_train[i,0]]:\n",
    "            user_likes_train[rec_train[i,0]].append(rec_train[i,2])\n",
    "\n",
    "# TODO: rename some things here?\n",
    "# make kg dictionaries\n",
    "name = ['test', 'train']\n",
    "heads_test, tails_test = {}, {}\n",
    "for i, kg in enumerate([kg_test, kg_train]):\n",
    "    for j, fact in enumerate(kg):\n",
    "        headkey = tuple(fact[1:])\n",
    "        tailkey = tuple(fact[:2])\n",
    "        if headkey in heads_test.keys():\n",
    "            heads_test[headkey].append(fact[0])\n",
    "        else:\n",
    "            heads_test[headkey] = [fact[0]]\n",
    "        if tailkey in tails_test.keys():\n",
    "            tails_test[tailkey].append(fact[2])\n",
    "        else:\n",
    "            tails_test[tailkey] = [fact[2]]\n",
    "    with open('datasets/ML_FB/kg_tail_{}.pkl'.format(name[i]), 'wb') as f:\n",
    "       pickle.dump(heads_test, f)   \n",
    "    with open('datasets/ML_FB/kg_head_{}.pkl'.format(name[i]), 'wb') as f:\n",
    "       pickle.dump(tails_test, f)\n",
    "\n",
    "np.save('datasets/ML_FB/rec_train.npy', rec_train, allow_pickle=True)\n",
    "np.save('datasets/ML_FB/rec_test.npy', rec_test, allow_pickle=True)\n",
    "np.save('datasets/ML_FB/kg_train.npy', kg_train, allow_pickle=True)\n",
    "np.save('datasets/ML_FB/kg_test.npy', kg_test, allow_pickle=True)\n",
    "\n",
    "with open('datasets/ML_FB/id2html.pkl', 'wb') as f:\n",
    "    pickle.dump(id2html_map, f)\n",
    "with open('datasets/ML_FB/rel_map.pkl', 'wb') as f:\n",
    "    pickle.dump(link_map, f)\n",
    "with open('datasets/ML_FB/user_likes_train.pkl', 'wb') as f:\n",
    "    pickle.dump(user_likes_train, f) \n",
    "with open('datasets/ML_FB/user_likes_test.pkl', 'wb') as f:\n",
    "    pickle.dump(user_likes_test, f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aaead76966e06fb07e848f83a22afdfdaeabf8dd265c367aa4dd9bab3bc5ff98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
