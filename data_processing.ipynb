{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movielens - Freebase Processing\n",
    "#### Create Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "\n",
    "# this is slow, use np.load() instead\n",
    "kg_path = 'datasets/www_data/www_data/Movielens/kg/train.dat'\n",
    "rec_path = 'datasets/www_data/www_data/Movielens/rs/ratings.txt'\n",
    "kg = np.genfromtxt(kg_path, delimiter='\\t', dtype=np.int32)\n",
    "rec = np.genfromtxt(rec_path, delimiter='\\t', dtype=np.int32)\n",
    "\n",
    "rec = rec[:,:3] # remove time col.\n",
    "rec[:,2] = rec[:,2] >= 4 # binary ratings, 0 if [0,3.5], 1 if [4, 5] \n",
    "rec = rec[rec[:,2] == 1] # select only positive ratings\n",
    "rec[:,2] = 47 # set redundant col to relationship 47\n",
    "rec = rec[:, [0,2,1]] # <user, likes, item> format\n",
    "\n",
    "# checkpoint: user and item format are still in ml id's\n",
    "# step 1: convert item id's first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_FB_IDS = np.max(kg) # total number of default kg pairs (# rel << # entities)\n",
    "\n",
    "# paths for converting data\n",
    "item2kg_path = 'datasets/www_data/www_data/Movielens/rs/i2kg_map.tsv'\n",
    "emap_path = 'datasets/www_data/www_data/Movielens/kg/e_map.dat'\n",
    "\n",
    "# maps movie lense id's to free base html links\n",
    "ml2fb_map = {}\n",
    "with open(item2kg_path) as f:\n",
    "    for line in f:\n",
    "        ml_id = re.search('(.+?)\\t', line)\n",
    "        fb_http = re.search('\\t(.+?)\\n', line)\n",
    "        \n",
    "        ml2fb_map.update({int(ml_id.group(1)) : fb_http.group(1)})\n",
    "\n",
    "# maps free base html links to free base id's (final format)\n",
    "fb2id_map = {}\n",
    "with open(emap_path) as f:\n",
    "    for kg_id, line in enumerate(f):\n",
    "        fb_http = re.search('\\t(.+?)\\n', line)\n",
    "        \n",
    "        fb2id_map.update({fb_http.group(1) : kg_id})\n",
    "\n",
    "# convert movielens id's to freebase id's\n",
    "new_ids = 0\n",
    "for i in range(rec.shape[0]):\n",
    "    if rec[i,2] in ml2fb_map: \n",
    "        # get correct freebase id from data\n",
    "        fb_http = ml2fb_map[rec[i,2]]\n",
    "        fb_id = fb2id_map[fb_http]\n",
    "    else:\n",
    "        # create new freebase id\n",
    "        new_ids += 1\n",
    "        fb_id = TOTAL_FB_IDS + new_ids\n",
    "\n",
    "        # add information to maps (for repeat values) \n",
    "        ml2fb_map.update({rec[i,2] : '<http:dummy/{}'.format(new_ids)})\n",
    "        fb2id_map.update({'<http:dummy/{}'.format(new_ids) : fb_id})\n",
    "    rec[i,2] = fb_id\n",
    "\n",
    "# checkpoint: now 'likes' and movies are in freebase id's\n",
    "# step #2: convert user id's into freebase id's\n",
    "NEW_MOVIE_IDS = new_ids\n",
    "\n",
    "umap_path = 'datasets/www_data/www_data/Movielens/rs/u_map.dat'\n",
    "\n",
    "# maps movielens user id's to freebase id's\n",
    "userid2fbid_map = {}\n",
    "new_ids = 0\n",
    "with open(umap_path) as f:\n",
    "    for line in f:\n",
    "\n",
    "        ml_id = re.search('\\t(.+?)\\n', line)\n",
    "        if int(ml_id.group(1)) in rec[:,0]:\n",
    "            new_ids += 1\n",
    "            userid2fbid_map.update({int(ml_id.group(1)) : TOTAL_FB_IDS + NEW_MOVIE_IDS + new_ids})\n",
    "\n",
    "# convert movielens user id's into freebase id's\n",
    "for i in range(rec.shape[0]):\n",
    "    rec[i,0] = userid2fbid_map[rec[i,0]]\n",
    "\n",
    "NEW_USER_IDS = new_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Dictionary for Triplet Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed 25 items\n"
     ]
    }
   ],
   "source": [
    "# create dictionary which maps id's to readable text\n",
    "fbid2word_map = {}\n",
    "\n",
    "# add users\n",
    "for i in range(NEW_USER_IDS):\n",
    "    fbid2word_map.update({TOTAL_FB_IDS + NEW_MOVIE_IDS + i + 1 : 'User {}'.format(i)})\n",
    "\n",
    "item_path = 'datasets/www_data/www_data/Movielens/rs/i_map.dat'\n",
    "movie_path = 'datasets/www_data/www_data/Movielens/rs/movies.csv'\n",
    "\n",
    "# converts short movielens ids to movielens id\n",
    "movie_count = 0\n",
    "shortml2ml_map = {}\n",
    "with open(item_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        ml_id = re.search('\\t(.+?)\\n', line)\n",
    "        shortml2ml_map.update({i : int(ml_id.group(1))})\n",
    "        movie_count += 1\n",
    "\n",
    "shortml2movie_map = {}\n",
    "with open(movie_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0: continue # skip first line\n",
    "        movie = re.search(',(.+?),', line)\n",
    "        shortml2movie_map.update({i-1 : movie.group(1)})\n",
    "\n",
    "# add movies to dictionary\n",
    "miss_list = []\n",
    "for i in range(movie_count):\n",
    "    ml_id = shortml2ml_map[i]\n",
    "    if ml_id in ml2fb_map:\n",
    "        fb_http = ml2fb_map[ml_id]\n",
    "    else:\n",
    "        # items not in dict because no ratings at 4 or above\n",
    "        miss_list.append(ml_id)\n",
    "    fb_id = fb2id_map[fb_http]\n",
    "    movie = shortml2movie_map[i]\n",
    "\n",
    "    fbid2word_map.update({fb_id : movie})\n",
    "print('missed {} items'.format(len(miss_list)))\n",
    "\n",
    "\n",
    "# relationship map\n",
    "link_map = {}\n",
    "link_path = 'datasets/www_data/www_data/Movielens/kg/r_map.dat'\n",
    "\n",
    "# make dict of relationship vals\n",
    "with open(link_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        link = re.search('/film.film.(.+?)>', line)\n",
    "        link_map.update({i : link.group(1).replace('_', ' ').capitalize()})\n",
    "link_map.update({47 : 'Likes'}) # add likes relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dictionary of User Likes Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_likes_map = {}\n",
    "for i in range(rec.shape[0]):\n",
    "    if rec[i,0] not in user_likes_map:\n",
    "        arr = [rec[i,2]]\n",
    "        user_likes_map.update({rec[i,0]: arr})\n",
    "    else:\n",
    "        if rec[i,2] not in user_likes_map[rec[i,0]]:\n",
    "            user_likes_map[rec[i,0]].append(rec[i,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "np.save('datasets/ML_FB/kg.npy', kg, allow_pickle=True)\n",
    "np.save('datasets/ML_FB/rec.npy', rec, allow_pickle=True)\n",
    "\n",
    "np.random.shuffle(rec)\n",
    "split = int(0.7*rec.shape[0])\n",
    "rec_train = rec[:split]\n",
    "rec_test = rec[split:]\n",
    "\n",
    "np.save('datasets/ML_FB/rec_train.npy', rec_train, allow_pickle=True)\n",
    "np.save('datasets/ML_FB/rec_test.npy', rec_test, allow_pickle=True)\n",
    "with open('datasets/ML_FB/item_map.pkl', 'wb') as f:\n",
    "    pickle.dump(fbid2word_map, f)\n",
    "with open('datasets/ML_FB/rel_map.pkl', 'wb') as f:\n",
    "    pickle.dump(link_map, f)\n",
    "with open('datasets/ML_FB/user_likes_map.pkl', 'wb') as f:\n",
    "    pickle.dump(user_likes_map, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.647126197814941\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "\n",
    "rec = np.load('datasets/ML_FB/rec.npy')\n",
    "kg = np.load('datasets/ML_FB/kg.npy')\n",
    "fin = np.concatenate((rec, kg), axis=0)\n",
    "\n",
    "t = time.time()\n",
    "np.random.shuffle(fin)\n",
    "print(time.time() - t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/ML_KG/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5ef983ff90f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/ML_KG/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\t{}\\t{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/ML_KG/train.txt'"
     ]
    }
   ],
   "source": [
    "# move numpy array's to .txt files that can be read into Simple process\n",
    "# we need train, test, valid\n",
    "# ratio: [70, 20, 10]\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "#kg = np.load('data/kg.npy')\n",
    "#rec = np.load('data/rec.npy')\n",
    "\n",
    "#data = np.concatenate((kg,rec), axis=0)\n",
    "##np.random.shuffle(data)\n",
    "\n",
    "#train_end = int(0.7 * data.shape[0])\n",
    "#test_end = int(0.2 * data.shape[0]) + train_end\n",
    "\n",
    "#f = open('datasets/ML_KG/train.txt', 'w+')\n",
    "#for i in range(train_end):\n",
    "    #f.write('{}\\t{}\\t{}\\n'.format(data[i,0], data[i,1], data[i,2]))\n",
    "#f.close()\n",
    "\n",
    "#f = open('datasets/ML_KG/test.txt', 'w+')\n",
    "#for i in range(train_end, test_end):\n",
    "    #f.write('{}\\t{}\\t{}\\n'.format(data[i,0], data[i,1], data[i,2]))\n",
    "#f.close()\n",
    "\n",
    "#trig = 0\n",
    "#f = open('datasets/ML_KG/valid.txt', 'w+')\n",
    "#for i in range(test_end, data.shape[0]):\n",
    "    #f.write('{}\\t{}\\t{}\\n'.format(data[i,0], data[i,1], data[i,2]))\n",
    "#f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
