{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# this is slow, use np.load() instead\n",
    "kg_path = 'datasets/www_data/www_data/Movielens/kg/train.dat'\n",
    "rec_path = 'datasets/www_data/www_data/Movielens/rs/ratings.txt'\n",
    "kg = np.genfromtxt(kg_path, delimiter='\\t', dtype=np.uint32)\n",
    "rec = np.genfromtxt(rec_path, delimiter='\\t', dtype=np.uint32)\n",
    "\n",
    "rec = rec[:,:3] # remove time col.\n",
    "rec[:,2] = rec[:,2] >= 4 # binary ratings, 0 if [0,3.5], 1 if [4, 5] \n",
    "rec = rec[rec[:,2] == 1] # select only positive ratings\n",
    "rec[:,2] = 47 # set redundant col to relationship 47\n",
    "rec = rec[:, [0,2,1]] # <user, likes, item> format\n",
    "\n",
    "# checkpoint: user and item format are still in ml id's\n",
    "# step 1: convert item id's first\n",
    "\n",
    "# save to np arrays for fast loading\n",
    "os.makedirs('data', exist_ok=True)\n",
    "np.save('data/kg.npy', kg, allow_pickle=True)\n",
    "np.save('data/rec.npy', rec, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52811    47 12262]\n",
      " [52811    47  2918]\n",
      " [52811    47 10554]\n",
      " [52811    47 12030]\n",
      " [52811    47  8640]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "TOTAL_FB_IDS = 52536 # total number of default kg pairs\n",
    "rec = np.load('data/rec.npy', allow_pickle=True)\n",
    "\n",
    "# paths for converting data\n",
    "item2kg_path = 'datasets/www_data/www_data/Movielens/rs/i2kg_map.tsv'\n",
    "emap_path = 'datasets/www_data/www_data/Movielens/kg/e_map.dat'\n",
    "\n",
    "# maps movie lense id's to free base html links\n",
    "ml2fb_map = {}\n",
    "with open(item2kg_path) as f:\n",
    "    for line in f:\n",
    "        ml_id = re.search('(.+?)\\t', line)\n",
    "        fb_http = re.search('\\t(.+?)\\n', line)\n",
    "        \n",
    "        ml2fb_map.update({int(ml_id.group(1)) : fb_http.group(1)})\n",
    "\n",
    "# maps free base html links to free base id's (final format)\n",
    "fb2id_map = {}\n",
    "with open(emap_path) as f:\n",
    "    for kg_id, line in enumerate(f):\n",
    "        fb_http = re.search('\\t(.+?)\\n', line)\n",
    "        \n",
    "        fb2id_map.update({fb_http.group(1) : kg_id})\n",
    "\n",
    "# convert movielens id's to freebase id's\n",
    "new_ids = 0\n",
    "for i in range(rec.shape[0]):\n",
    "    if rec[i,2] in ml2fb_map: \n",
    "        # get correct freebase id from data\n",
    "        fb_http = ml2fb_map[rec[i,2]]\n",
    "        fb_id = fb2id_map[fb_http]\n",
    "    else:\n",
    "        # create new freebase id\n",
    "        new_ids += 1\n",
    "        fb_id = TOTAL_FB_IDS + new_ids\n",
    "\n",
    "        # add information to maps (for repeat values) \n",
    "        ml2fb_map.update({rec[i,2] : '<http:dummy/{}'.format(new_ids)})\n",
    "        fb2id_map.update({'<http:dummy/{}'.format(new_ids) : fb_id})\n",
    "    rec[i,2] = fb_id\n",
    "\n",
    "# checkpoint: now 'likes' and movies are in freebase id's\n",
    "# step #2: convert user id's into freebase id's\n",
    "NEW_MOVIE_IDS = new_ids\n",
    "umap_path = 'datasets/www_data/www_data/Movielens/rs/u_map.dat'\n",
    "\n",
    "# maps movielens user id's to freebase id's\n",
    "userid2fbid_map = {}\n",
    "new_ids = 0\n",
    "with open(umap_path) as f:\n",
    "    for line in f:\n",
    "        new_ids += 1\n",
    "        ml_id = re.search('\\t(.+?)\\n', line)\n",
    "        userid2fbid_map.update({int(ml_id.group(1)) : TOTAL_FB_IDS + NEW_MOVIE_IDS + new_ids})\n",
    "        \n",
    "# convert movielens user id's into freebase id's\n",
    "for i in range(rec.shape[0]):\n",
    "    rec[i,0] = userid2fbid_map[rec[i,0]]\n",
    "\n",
    "NEW_USER_IDS = new_ids\n",
    "print(rec[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed 25 items\n"
     ]
    }
   ],
   "source": [
    "fbid2word_map = {}\n",
    "\n",
    "# add users\n",
    "for i in range(NEW_USER_IDS):\n",
    "    fbid2word_map.update({TOTAL_FB_IDS + NEW_MOVIE_IDS + i + 1 : 'User {}'.format(i)})\n",
    "\n",
    "\n",
    "item_path = 'datasets/www_data/www_data/Movielens/rs/i_map.dat'\n",
    "movie_path = 'datasets/www_data/www_data/Movielens/rs/movies.csv'\n",
    "\n",
    "# TODO: check this code... could be problem in dataset...\n",
    "# converts movielens ids to movielens id\n",
    "movie_count = 0\n",
    "shortml2ml_map = {}\n",
    "with open(item_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        ml_id = re.search('\\t(.+?)\\n', line)\n",
    "        shortml2ml_map.update({i : int(ml_id.group(1))})\n",
    "        movie_count += 1\n",
    "\n",
    "shortml2movie_map = {}\n",
    "with open(movie_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0: continue # skip first line\n",
    "        movie = re.search(',(.+?),', line)\n",
    "        shortml2movie_map.update({i-1 : movie.group(1)})\n",
    "\n",
    "# add movies to dictionary\n",
    "miss_list = []\n",
    "for i in range(movie_count):\n",
    "    ml_id = shortml2ml_map[i]\n",
    "    if ml_id in ml2fb_map:\n",
    "        fb_http = ml2fb_map[ml_id]\n",
    "    else:\n",
    "        # items not in dict because no ratings at 4 or above\n",
    "        miss_list.append(ml_id)\n",
    "    fb_id = fb2id_map[fb_http]\n",
    "    movie = shortml2movie_map[i]\n",
    "\n",
    "    fbid2word_map.update({fb_id : movie})\n",
    "print('missed {} items'.format(len(miss_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save for fast loading\n",
    "np.save('data/rec.npy', rec, allow_pickle=True)\n",
    "with open('data/dict.pkl', 'wb') as f:\n",
    "    pickle.dump(fbid2word_map, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
